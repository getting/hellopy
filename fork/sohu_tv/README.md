问题1: 爬取tv.sohu.com的页面, 提取视频相关信息，不可用爬虫框架完成

需求:

* 做到最大可能的页面覆盖率
* 选择重要的信息进行存储
* 选择合适的数据存储方式，便于后续使用
* 可通过参数限制要抓取视频信息的数目
* 要用多线程方式完成抓取
* 反防抓取策略
* *分布式支持
* *崩溃后进度恢复

星号部分为加分项, 可只给出设计思路 ...



### 采用广度优先算法

从一个入口(首页）地址开始，将地址加入队列，并存储到已抓取的url集合中，将存储在队列中的地址取出并找出对应页面全部url地址，
将得到的地址与已经抓取的url集合进行对比，如果url集合中尚不存在，加入集合，并加入队列。重复操作，知道寻找到所有链接。

对于得到的链接如果符合视频页规则，则加入视频信息提取队列，进行视频信息提取。



### 只抓取单独的视频页，不抓取分类和剧集等页面信息

对于视频页主要有两种类型：

官方发布的视频：http://tv.sohu.com/20140304/n395970803.shtml
日期 + 编号

用户上传的视频：http://my.tv.sohu.com/us/200430155/63582287.shtml
用户id + 编号


body部分不同频道结构不够统一，对于一般的视频信息采集，页面head部分的信息已经足够丰富（目前也只采集head部分)。
如果需要更详尽的信息大概主要可从下面这些片段提取信息。

    <div class="infoBox cfix  id=info">
    <div class="info info-con">
    <div class="area cfix" id="content">



### 数据采用MongoD存储

主要有两个集合：
url   存储已经收集到的地址：主要用于比对和去重，防止重复抓取B

* url: 原始链接地址
* id: 原始链接地址的hash值，用于快速比对


tv    用于存储获取的视频信息

数据存储在MongoDB，为提高性能对于已经抓取的url可以考虑用redis存储和进行去重比对。广度优先可能会采集很多重复的url，
在数据库中已知url变多时，从中进行查询和对比可能会很慢，可以将类似首页和主要濒道的url等最容易出现重复的部分单独存储在一个列表中，
在去重对比时先对比这个小的列表，如果重复则不再需要查询数据库。


### 可通过参数限制要抓取视频信息的数目
参数可以通过 video_num 设置


### 反防抓取策略：
通过设置设置User-Agent等实现简单的反防抓取，如果需要可以添加代理设置。


### 分布式支持

设置一台设备为调度中心，所有爬虫将获得的url提交到中心控制器，中心控制器负责过滤，并将待处理的视频url分发到视频信息提取爬虫。
可按url类型、时期等进行切分。


### 崩溃后的恢复？

所有已经遍历的地址存储在数据库中，在一个地址被成功处理后可以做一条标记，在崩溃后将未标记部分重新加入队列，从上次结束的位置继续搜索。


### 爬虫测试

在4小时内抓取独立url地址超过10万，视频信息条目超过8万。数据参见data目录内。


### 遇到的问题：

* 指定参数限制要抓取视频信息的数目，在达到抓取数目时无法自行终止(多线程?)。
* 目前的程序在采集url的同时进行视频信息的提取，对于速度难以控制，或许将两者分开进行效果会更好些。


### 视频总数估算

site:tv.sohu.com

Baidu: 83,100,000
Google：17,300,000

30,000,000 ?
